---
title: "Data Mining Exercise 2"
author: "Kylie Taylor"
date: "3/15/2019"
output: pdf_document
---

## Saratoga House Prices



```{r, include=FALSE}
library(tidyverse)
library(mosaic)
data(SaratogaHouses)

# Split into training and testing sets
n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
saratoga_train = SaratogaHouses[train_cases,]
saratoga_test = SaratogaHouses[test_cases,]
```

The output below reflects work we have done to determine which variables included in the Saratoga Houses data contained in the 'mosaic' package in R. The first model that we ran was the medium length model from class that is the model we are trying to preform better than. This model estimates house prices using the variables "lotSize", "age", "livingArea", "pctCollege", "bedrooms", "bathrooms", "fireplaces", "rooms", "heating", "fuel", and "centralAir". The factors found to be most significant in estimating price are "lotSize", "livingArea", "bedrooms", "bathrooms", "rooms", and "centralAir".

The output from the medium length model in class is below. We see that there is an $R^2$ = 0.55 and a RMSE of 66,767 (averaged over 1,000 sampled RMSE's).  


```{r, echo=FALSE}	
set.seed(123990)
# Fit to the training data
lm.med = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
stargazer::stargazer(lm.med, type = "text")

rmse = function(y, yhat) {sqrt(mean((y - yhat)^2))}

#averaging over train/test splits
rmse_vals = do(1000)*{
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  # Fit to the training data
  lm2 = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
  yhat_test2 = predict(lm2, saratoga_test)
  rmse(saratoga_test$price, yhat_test2)
}

colMeans(rmse_vals)
```



In an attempt to find a way to out-preform the model above, we used the package 'leaps' to run a best subsets on Saratoga houses. The best subsets algorithm cycles through all variables of interst in a data frame and combines them into seperate regressions, then (in this case) compares the $R^2$ values of the many linear models that were generated by the algorithm. Below is a plot of the variables that best subsets suggests with the corresponding adjuested $R^2$. We see that if we include all the variables, the adjusted $R^2$ is about the same as if we were to leave a few superfluous variables, like "fireplace". A plot of adjusted $R^2$ on the y-axis and variables on the x-axis is below.


```{r, echo=FALSE}
library(leaps)
regsubsets.out <-
    regsubsets(price ~ . - landValue,
               data = SaratogaHouses,
               nbest = 1,       # 1 best model for each number of predictors
               nvmax = NULL,    # NULL for no limit on number of variables
               force.in = NULL, force.out = NULL,
               method = "exhaustive")

plot(regsubsets.out, scale = "adjr2", main = "Adjusted R^2")
```



We created two models to compete with the medium model built in class, while also letting the best subsets inform us that there is no variable that should clearly be left out. Before running any models, we made the decison to remove the variables "landValue" from this analysis, as it is measuring almost the same thing as price. It would not be "fair" to include in any models, if our ultimate goal is to determine which factor explains house prices the best. 

The first model we ran inculdes all the variables. The second model we ran includes all variables except "fireplace" and "sewer". The third and final model we tested was the same as the second plus interaction terms of "bedrooms$*$bathrooms", "age$*$heating" and "age$*$lotSize".

After averaging over 500 sampled RMSE's from the three models, we found that the second and the third model preform the best with equal RMSE's of 64,485.69.


```{r, echo=FALSE}
set.seed(129)
rmse = function(y, yhat) {sqrt(mean((y - yhat)^2))}

rmse_vals2 = do(500)*{
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  
  lm.med = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
  lm <- lm(price ~. -landValue , data = saratoga_train)
  lm1 <- lm(price ~  lotSize + age + pctCollege + livingArea + bedrooms + bathrooms + rooms + heating + fuel + waterfront + newConstruction + centralAir, data = saratoga_train)
  lm3 <- lm(price ~  lotSize + age + pctCollege + livingArea + bedrooms + bathrooms + rooms + heating + fuel + waterfront + newConstruction + centralAir + bedrooms*bathrooms + age*heating + age*lotSize, data = saratoga_train)
  yhat_testmed = predict(lm.med, saratoga_test)
  yhat_test = predict(lm, saratoga_test)
  yhat_test1 = predict(lm1, saratoga_test)
  yhat_test3 = predict(lm3, saratoga_test)
  c(rmse(saratoga_test$price, yhat_testmed), rmse(saratoga_test$price, yhat_test), rmse(saratoga_test$price, yhat_test1), rmse(saratoga_test$price, yhat_test1))
}

colMeans(rmse_vals2)

```



We made the conclusion that the third model is the best, because the $R^2$ of the third model is the highest out of the three with an adjusted $R^2$ of 0.563. Therefore, this is the model we will be using for our analysis.

The linear model reveals that "lotSize", "livingArea", "bedrooms", "bathrooms", "rooms", "heating: water/steam", "waterFront", "newConstruction", "airCentral", "age$*$heating:water/steam", and "age$*$lotSize" are significant in estimating the value of a house in Saratoga.


```{r}
lm.med <- lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
lm <- lm(price ~. -landValue , data = saratoga_train)
lm1 <- lm(price ~  lotSize + age + pctCollege + livingArea + bedrooms + bathrooms + rooms + heating + fuel + waterfront + newConstruction + centralAir, data = saratoga_train)
lm3 <- lm(price ~  lotSize + age + pctCollege + livingArea + bedrooms + bathrooms + rooms + heating + fuel + waterfront + newConstruction + centralAir + bedrooms*bathrooms + age*heating + age*lotSize, data = saratoga_train)
stargazer::stargazer(lm, lm1, lm3, type = "text")
```



After determining that the third model was the best, we ran a KNN regression on the same variables. In order to do this, we stanadardized the variables. This resulted in the RMSEs being on a much different scale (the z-scale to be exact) than the RMSEs from the linear models. We ran KNN's on 7 different K values; 5, 20, 50, 70, 100, 200, and 300. 

A KNN model with 5 nearest neighors appears to have the smallest RMSE out of the 7 models tested. This is an interesting tradeoff, becuase a low K results in low variance, but high bias. 

One way to compare if the KNN model does better than the linear model is to standardize the variables in the linear models and compare the RMSE's between the two methods. 
The standardized RMSE of the chosen linear model is 0.6536, this is significantly bigger than the RMSE for a KNN with 5 nearest neighbors of 0.3692. For this reason, we know that KNN is better preforming and therefore should be used.  


```{r, echo=FALSE}
library(readr)
library(psycho)
library(tidyverse)
library(dplyr)
library(FNN)

SaratogaHouses$I1 <- SaratogaHouses$bedrooms*SaratogaHouses$bathrooms
SaratogaHouses$gas <- ifelse(SaratogaHouses$fuel == "gas", 1, 0)
SaratogaHouses$electric <- ifelse(SaratogaHouses$fuel == "electric", 1, 0)
SaratogaHouses$oil <- ifelse(SaratogaHouses$fuel == "oil", 1, 0)
SaratogaHouses$septic <- ifelse(SaratogaHouses$sewer == "septic", 1, 0)
SaratogaHouses$sewerpublic <- ifelse(SaratogaHouses$sewer == "public/commercial", 1, 0)
SaratogaHouses$nosewer <- ifelse(SaratogaHouses$sewer == "none", 1, 0)
SaratogaHouses$YESwaterfront <- ifelse(SaratogaHouses$waterfront == "Yes", 1, 0)
SaratogaHouses$NOwaterfront <- ifelse(SaratogaHouses$waterfront == "No", 1, 0)
SaratogaHouses$YESnewConstruct <- ifelse(SaratogaHouses$newConstruction == "Yes", 1, 0)
SaratogaHouses$NOTnewConstruct <- ifelse(SaratogaHouses$newConstruction == "No", 1, 0)
SaratogaHouses$YESCentralAir <- ifelse(SaratogaHouses$centralAir == "Yes", 1, 0)
SaratogaHouses$NOCentralAir <- ifelse(SaratogaHouses$centralAir == "No", 1, 0)
SaratogaHouses$heatingsteam <- ifelse(SaratogaHouses$heating == "hot water/steam", 1,0)

SaratogaHouses$ageheatingsteam <- SaratogaHouses$age*SaratogaHouses$heatingsteam
SaratogaHouses$agelotSize <- SaratogaHouses$age*SaratogaHouses$lotSize
SaratogaHouses$bedbath <- SaratogaHouses$bedrooms*SaratogaHouses$bathrooms


SH <- SaratogaHouses %>% psycho::standardize()

rmse = function(y, yhat) {sqrt(mean((y - yhat)^2))}

K = 100

rmsevals = do(K)*{
  n = nrow(SH)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  SH_train = SH[train_cases,]
  SH_test = SH[test_cases,]
  x_train = select(SH_train, c(-fireplaces, -sewer, -landValue, -heating, -fuel, -waterfront, -newConstruction, -centralAir))
  y_train = select(SH_train, price)
  x_test = select(SH_test, c(-fireplaces, -sewer, -landValue, -heating, -fuel, -waterfront, -newConstruction, -centralAir))
  y_test = select(SH_test, price)
  knn5 = knn.reg(train=x_train, test = x_test, y = y_train, k=5)
  ypred_knn5 = knn5$pred
  knn20 = knn.reg(train=x_train, test = x_test, y = y_train, k=20)
  ypred_knn20 = knn20$pred
  knn50 = knn.reg(train=x_train, test = x_test, y = y_train, k=50)
  ypred_knn50 = knn50$pred
  knn70 = knn.reg(train=x_train, test = x_test, y = y_train, k=70)
  ypred_knn70 = knn70$pred
  knn100 = knn.reg(train=x_train, test = x_test, y = y_train, k=100)
  ypred_knn100 = knn100$pred
  knn200 = knn.reg(train=x_train, test = x_test, y = y_train, k=200)
  ypred_knn200 = knn200$pred
  knn300 = knn.reg(train=x_train, test = x_test, y = y_train, k=300)
  ypred_knn300 = knn300$pred
  c(rmse(SH_test$price, ypred_knn5), rmse(SH_test$price, ypred_knn20), rmse(SH_test$price, ypred_knn50), rmse(SH_test$price, ypred_knn70), rmse(SH_test$price, ypred_knn100), rmse(SH_test$price, ypred_knn200), rmse(SH_test$price, ypred_knn300))
}

colMeans(rmsevals)
NN <- c(5, 20, 50, 70 ,100, 200, 300)
plot(colMeans(rmsevals) ~ NN, main = "RMSE vs number of neighbors", ylab="RMSE", xlab = "Number of Neighbors", type = 'l')
```

```{r}
rmse_vals.std = do(500)*{
  n = nrow(SH)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  SH_train = SH[train_cases,]
  SH_test = SH[test_cases,]
  lm3 <- lm(price ~  lotSize + age + pctCollege + livingArea + bedrooms + bathrooms + rooms + heating + fuel + waterfront + newConstruction + centralAir + bedrooms*bathrooms + age*heating + age*lotSize, data = SH_train)
  yhat_test3 = predict(lm3, SH_test)
  rmse(SH_test$price, yhat_test3)
}

colMeans(rmse_vals.std)
```



To conclude what these models have told us, the price of a house is best determined by lot size, age, proximity to a college, size of living area, number of bedrooms, number of bathrooms, number of rooms, if there is water/steam heating, type of fuel to the house, if its waterfront, if its new construction, if there is central air, the number of bedrooms times the number of bathrooms, age times if there is water/steam heating, and age times the lot size. We suggest that the local taxing authority should use a KNN model in contrast to a linear model to predict the prices of homes in Saratoga, NY, as a KNN model has the lowest out of sample RMSEs. To be specific, our analysis found that a KNN with 5 nearest neighbors has the lowest RMSE out of all the KNN models we tested. 



##Hospital Audit

#First question: are some radiologists more clinically conservative than others in recalling patients, holding patient risk factors equal?





#Second question: when the radiologists at this hospital interpret a mammogram to make a decision on whether to recall the patient, does the data suggest that they should be weighing some clinical risk factors more heavily than they currently are?


```{r, include =FALSE}
library(pander)
library(ggplot2)
brca <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/brca.csv", header=TRUE)
```

```{r, echo=FALSE}
pander(summary(brca))
```



```{r, echo=FALSE}
pander(aggregate(list(Cancer =brca$cancer), list(Radiologist = brca$radiologist, Recall = brca$recall), mean))
```


```{r, echo=FALSE}
library(stats)
set.seed(20)
n = nrow(brca)
d = ncol(brca)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train

train_ind = sort(sample.int(n, n_train, replace=FALSE))
test_cases = setdiff(1:n, train_ind)
brca_train = brca[train_ind,]
brca_test = brca[-train_ind,]

glm0 <- glm(cancer ~ recall -1, data = brca_train, family=binomial)
glm1 <- glm(cancer ~ . -1, data=brca_train , family = binomial)
glm2 <- glm(cancer ~ ., data=brca_train , family = binomial)
stargazer::stargazer(glm0, glm1, glm2, type= "text")
```




```{r, echo=FALSE}
# sanity check? can we reconstruct the in-sample deviance?
probhat2_train = predict(glm1, newdata=brca_train)

# pick out the correct row-column pairs from the probhat matrix
rc_train = cbind(seq_along(brca_train$cancer), brca_train$cancer)
head(rc_train)
deviance(glm1)

```






```{r, echo=FALSE}
# Let's use this to compare our three models on the testing set

# here's a generic function for calculating out-of-sample deviance
dev_out = function(y, probhat) {
  rc_pairs = cbind(seq_along(y), y)
  -2*sum(log(probhat[rc_pairs]))
}

# check
dev_out(fgl_train$type, probhat2_train)

# make predictions
probhat1_test = predict(ml1, newdata=fgl_test, type='probs')
probhat2_test = predict(ml2, newdata=fgl_test, type='probs')
probhat3_test = predict(ml3, newdata=fgl_test, type='probs')

# Calculate deviance
dev_out(fgl_test$type, probhat1_test)
dev_out(fgl_test$type, probhat2_test)
dev_out(fgl_test$type, probhat3_test)

# out-of-sample classification error rate
yhat3_test = predict(ml3, newdata=fgl_test, type='class')
conf3 = table(fgl_test$type, yhat3_test)
conf3
sum(diag(conf3))/n_test
```



##Viral Articles



```{r, include=FALSE}
library(readr)
library(FNN)
library(tidyverse)
library(dplyr)
onlinenews <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/online_news.csv", header = TRUE)
onlinenews$viral <- ifelse(onlinenews$shares >= 1400, 1,0)
ON <- na.omit(onlinenews)
```

```{r, echo=FALSE}
ON$viral <- as.numeric(ON$shares >= 1400)
head(ON$viral)

ON$Tuesday <- ifelse(ON$weekday_is_tuesday == 1, 2, 0)
ON$Wednesday <- ifelse(ON$weekday_is_wednesday == 1, 3, 0)
ON$Thursday <- ifelse(ON$weekday_is_thursday == 1, 4, 0)
ON$Friday <- ifelse(ON$weekday_is_friday == 1, 5, 0)
ON$Saturday <- ifelse(ON$weekday_is_saturday == 1, 6, 0)
ON$Sunday <- ifelse(ON$weekday_is_sunday == 1, 7, 0)

onlinenews$Day <- cbind(ON$weekday_is_monday, ON$Tuesday , ON$Wednesday, ON$Thursday, ON$Friday, ON$Saturday, ON$Sunday)

boxplot(ON$shares~ON$Day)



# Testing Linear model on full dataset
lm_small = lm(shares~n_tokens_title+num_imgs+num_videos+average_token_length, data=onlinenews)
coef(lm_small)

lm_all = lm(shares~(.-url), data=onlinenews)
coef(lm_all)


## TRAIN/TEST
n = nrow(onlinenews)

n_train=round(.8*n)
n_test = n - n_train

train_ind = sample.int(n,n_train, replace=FALSE)

d_train = onlinenews[train_ind, ]
d_test = onlinenews[-train_ind, ]

d_test = arrange(d_test, shares)

### Seperation of training and testing sets

x_train = select(d_train, c(num_imgs, num_videos, average_token_length))
y_train = select(d_train, viral)

x_test = select(d_test, c(num_imgs, num_videos, average_token_length))
y_test = select(d_test, viral)



#### Testing linear models on training dataset

lm_small_train =lm(shares~ num_imgs + num_videos + average_token_length, data=d_train)

lm_all_train = lm(shares~(.-url), data=d_train)

#####KNN 300
knn300 = knn.reg(train=x_train, test = x_test, y = y_train, k=300)

####### RSME function

rmse = function(y, ypred) {sqrt(mean(data.matrix((y-ypred)^2)))}

######## Prediction

ypred_lmsmall = predict(lm_small_train, x_test)
ypred_lmall = predict(lm_all_train, x_test)
ypred_knn300 = knn300$pred

```


```{r, echo=FALSE}
library(leaps)
regsubsets.out <-
    regsubsets(shares ~ . - url - viral,
               data = onlinenews,
               nbest = 1,       # 1 best model for each number of predictors
               nvmax = NULL,    # NULL for no limit on number of variables
               force.in = NULL, force.out = NULL,
               method = "exhaustive")
regsubsets.out


```



```{r, echo=FALSE}
summary.out <- summary(regsubsets.out)
as.data.frame(summary.out$outmat)
plot(regsubsets.out, scale = "adjr2", main = "Adjusted R^2")
```



```{r, echo=FALSE}
library(bestglm)

onlinenews.for.bestglm <- within(onlinenews, {
    url   <- NULL        # Delete
    viral  <- NULL
    shares  <- NULL
    y    <- viral         # bwt into y
    viral  <- NULL        # Delete bwt
})

## Reorder variables
#onlinenews.for.bestglm <-
    #onlinenews.for.bestglm[, c("age","lwt","race.cat","smoke","preterm","ht","ui","ftv.cat","y")]

res.bestglm <-
    bestglm(Xy = onlinenews.for.bestglm,
            family = gaussian,
            IC = "AIC",                 # Information criteria for
            method = "exhaustive")

```


```{r}

n = nrow(onlinenews)
n_train = round(0.7*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
news_train = onlinenews[train_cases,]
news_test = onlinenews[test_cases,]

## Fitting models on train
lm1 = lm(shares ~ average_token_length + num_imgs + num_videos + num_hrefs + weekday_is_saturday, data=news_train)
lm2 = lm(shares ~ num_hrefs + is_weekend + global_rate_negative_words + data_channel_is_bus + self_reference_avg_sharess + data_channel_is_world + data_channel_is_entertainment + num_keywords + avg_negative_polarity, data=news_train)
lm3 = lm(shares ~ n_tokens_title + num_hrefs + weekday_is_monday + global_rate_positive_words + avg_negative_polarity + max_negative_polarity + is_weekend, data=news_train)

#in sample
phat_train1 = predict(lm1, news_train) 
phat_train2 = predict(lm2, news_train) 
phat_train3 = predict(lm3, news_train) 

yhat_train1 = ifelse(phat_train1 >= 1400, 1,0)
yhat_train2 = ifelse(phat_train2 >= 1400, 1,0)
yhat_train3 = ifelse(phat_train3 >= 1400, 1,0)

#in sample performance
confusion_in1 = table(y = news_train$viral, yhat = yhat_train1)
confusion_in2 = table(y = news_train$viral, yhat = yhat_train2)
confusion_in3 = table(y = news_train$viral, yhat = yhat_train3)

confusion_in1
confusion_in2
confusion_in3

sum(diag(confusion_in1))/sum(confusion_in1)
sum(diag(confusion_in2))/sum(confusion_in2)
sum(diag(confusion_in3))/sum(confusion_in3)

#out of sample
phat_test1 <- predict(lm1, news_test)
phat_test2 <- predict(lm2, news_test)
phat_test3 <- predict(lm3, news_test)

yhat_test1 = ifelse(phat_test1 > 1400, 1,0)
yhat_test2 = ifelse(phat_test2 > 1400, 1,0)
yhat_test3 = ifelse(phat_test3 > 1400, 1,0)

#out of sample performance
confusion_out1 = table(y = news_test$viral, yhat = yhat_test1)
confusion_out2 = table(y = news_test$viral, yhat = yhat_test2)
confusion_out3 = table(y = news_test$viral, yhat = yhat_test3)

confusion_out1
confusion_out2
confusion_out3

sum(diag(confusion_out1))/sum(confusion_out1)
sum(diag(confusion_out2))/sum(confusion_out2)
sum(diag(confusion_out3))/sum(confusion_out3)
```







